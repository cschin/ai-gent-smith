states = [
"StandBy",
"Triage",
"Retrieval",
"Generate",
"Finish"]

transitions = [
["StandBy",
"Triage"],
["Triage",
"Retrieval"],
["Retrieval",
"Generate"],
["Generate",
"Finish"]]

initial_state = "StandBy"
system_prompt = ""
fsm_prompt = "" 
summary_prompt = ""

[state_prompts.StandBy]
system = ""
#fsm = """JUST output a json string {"next_state": "Retrieval"}"""

[state_prompts.Triage]
system = """
You have the gift to tell people's background from some simple question.
You are given a question from the user, and you can figure his or her background from the question.
There are a number of possibility of the backgrounds, you can choose one of them.
Here is the question: <question> {{ task }} </question>

Here is the list of the backgrounds:
1. A businessman or a CEO, the question is about business, company, or money.
2. A regulatory consultant, the question is about law, regulation, or policy.
3. A scientist, the question is about science, technology, or research.
4. A customer service representative, the question is about customer service, product, or service.

If, in the question, the user mentioned a particular background, you need to choose the 
background according to what the user mentioned.

Now, you can start with:

I think the you might be a .... because the question is about ....

or 

I think you are a ... because you said ....

"""


[state_prompts.Retrieval]
system = ""
#fsm = """JUST output a json string {"next_state": "Generate"}"""

[state_prompts.Generate]
system = """
You are an expert in the field of FDA cosmetic guidances. You are given a question or a task from the user, and you 
need to provide an answer to the user's question and pefrom the task.

Here is the question or the task: <question> {{ task }} </question>

You are given a set of reference material in the <CONTEXT>...</CONTEXT>
section, use those to construct an answer to the user's question. 

Here is the context section
 <CONTEXT> 
 {{ context }} 
 </CONTEXT>
 
 You should provide the answer according the questioner's background: {{ background }}

For scientists, you can provide a detailed explanation of the science behind the question.
For business people, you can provide a business perspective on the question.
For regulatory consultants, you can provide a regulatory perspective on the question.
For customer service representatives, you can provide a customer service perspective on the question.

Output your response in markdown format with the following format:

<markdown>

start with 

I think you might be a ... because the question is about ...

or

As a ..., you might be interested in ...

then

Here I provide the answer to your question....

or 

Here is what you can use ...

### References

	- Document title 1
     
     - > context 1 (in block quote)
  
     	explanation
  
  	 - > context 2 (in block quote)
  
  		explanation
     
     - > ...
     
  - Document title 2
     
     - > context (in block quote) 
     
      explanation

</markdown>

Please make sure you display the exact copy of the relevant and non-redundant 
reference and context. The context should be 100% identical from those provided 
to you in the contexts within the section <CONTEXT>...</CONTEXT> below. You 
always provide a one-sentence explanation of why it is relevant to the question.

 
 """



fsm = """JUST output a json string {"next_state": "Finish"}"""

[state_config.StandBy]
# show how to use code/tool to drive the state transition, you can change ti the output "Generate" state
# to see the effect
fsm_code = """
print("Triage")"""

# don't make chat request but making the fsm transition request
disable_llm_request = true

[state_config.Triage]
use_task = true
save_to = ["background",]

[state_config.Retrieval]
execute_code = true
use_task = true
disable_llm_request = true # pure code execution 
# Retrieval.save_execution_output = true 
save_to_context = true # for RAG
# execute code from this content
code = """
import json
import requests

task = \"\"\"{{task}}\"\"\"

def query_for_chunks(query, asset_id, top_k=8, threshold=0.65):
    url = "http://host.docker.internal:8080/api/service/query_for_chunks"
    
    payload = {
        "query": query,
        "asset_id": asset_id,
        "top_k": top_k,
        "threshold": threshold
    }
    
    headers = {
        "Content-Type": "application/json"
    }
    
    response = requests.post(url, json=payload, headers=headers)
    
    if response.status_code == 200:
        return response.json()
    else:
        return f"Error: {response.status_code}, {response.text}"


if __name__ == "__main__":
    query = task
    asset_id = 1
    state_name = \"\"\"{{state_name}}\"\"\"
    print("state_name", state_name)
    result = query_for_chunks(query, asset_id)
    if result["message"] == "succeed":
        for d in result["data"]:
            print()
            print("## title: ", d["chunk"]["title"])
            print("similarity: ", d["similarity"])
            print(d["chunk"]["text"])
            print()

"""

[state_config.Generate]
#save_to_context = true
use_task = true
use_context = true
use_memory = [["background", 1]]
ignore_messages = true

[state_config.Finish]
# don't make chat request but making the fsm transition request
disable_llm_request = true
